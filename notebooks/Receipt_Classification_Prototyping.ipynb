{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- ENFOQUE 1: CLÁSICO (TF-IDF) ---\n",
    "def train_classic_model(df, text_col, label_col):\n",
    "    \"\"\"Entrena un Random Forest con TF-IDF.\"\"\"\n",
    "    print(\">>> Entrenando Modelo Clásico (TF-IDF + Random Forest)...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[text_col], df[label_col], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(ngram_range=(1, 3), max_features=5000)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return pipeline, {\n",
    "        'accuracy': acc,\n",
    "        'time': train_time,\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'X_test': X_test,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "\n",
    "# --- ENFOQUE 2: MODERNO (TRANSFORMERS) ---\n",
    "def get_transformer_embeddings(text_list, model_name='distilbert-base-uncased', batch_size=32):\n",
    "    \"\"\"\n",
    "    Genera embeddings usando un modelo pre-entrenado (Feature Extraction).\n",
    "    No hacemos fine-tuning completo para mantenerlo ligero en el prototipo.\n",
    "    \"\"\"\n",
    "    print(f\">>> Generando Embeddings con {model_name}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Mover a GPU si está disponible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Procesar por lotes para no saturar memoria\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch_texts = text_list[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                           max_length=64, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # Usamos el token CLS (primera posición) como vector de la frase\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "def train_transformer_head(embeddings, labels):\n",
    "    \"\"\"Entrena un clasificador ligero (Logistic Reg) sobre los embeddings.\"\"\"\n",
    "    print(\">>> Entrenando Clasificador sobre Embeddings...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        embeddings, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Usamos Regresión Logística porque los embeddings ya son linealmente separables\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return clf, {\n",
    "        'accuracy': acc,\n",
    "        'time': train_time, # Nota: Esto no incluye el tiempo de generación de embeddings\n",
    "        'report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eda455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generar Datos Sintéticos \"Sucios\"\n",
    "print(\"Generando 5,000 tickets sintéticos...\")\n",
    "df = generate_dataset(n_samples=5000)\n",
    "\n",
    "# 2. Preprocesamiento\n",
    "df['clean_text'] = df['receipt_text'].apply(normalize_text)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(\"\\nEjemplos de Datos:\")\n",
    "display(df[['receipt_text', 'clean_text', 'category']].sample(5))\n",
    "\n",
    "# Ver balanceo de clases\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(y=df['category'], palette='viridis')\n",
    "plt.title(\"Distribución de Categorías\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPERIMENTO 1: Enfoque Clásico (TF-IDF + Random Forest) ---\n",
    "model_classic, metrics_classic = train_classic_model(df, 'clean_text', 'category')\n",
    "\n",
    "print(f\"\\nAccuracy Clásico: {metrics_classic['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2528cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPERIMENTO 2: Enfoque Moderno (DistilBERT Embeddings) ---\n",
    "# Paso A: Convertir texto a vectores densos (Embeddings)\n",
    "embeddings = get_transformer_embeddings(df['clean_text'].tolist())\n",
    "\n",
    "# Paso B: Entrenar clasificador sobre los vectores\n",
    "model_dl, metrics_dl = train_transformer_head(embeddings, df['category'])\n",
    "\n",
    "print(f\"\\nAccuracy Deep Learning: {metrics_dl['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de Trade-offs\n",
    "results = pd.DataFrame({\n",
    "    'Modelo': ['Classic (TF-IDF)', 'Modern (DistilBERT)'],\n",
    "    'Accuracy': [metrics_classic['accuracy'], metrics_dl['accuracy']],\n",
    "    # Nota: Para DL sumamos un estimado de tiempo de inferencia para ser justos en la comparación\n",
    "    'Complexity': ['Low (CPU)', 'High (Need GPU)'] \n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gráfico 1: Precisión\n",
    "sns.barplot(x='Modelo', y='Accuracy', data=results, ax=ax[0], palette='Blues')\n",
    "ax[0].set_ylim(0, 1.1)\n",
    "ax[0].set_title('Comparación de Precisión')\n",
    "for i, v in enumerate(results['Accuracy']):\n",
    "    ax[0].text(i, v + 0.02, f\"{v:.1%}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Gráfico 2: Matriz de Confusión del mejor modelo (Supongamos DL)\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# Necesitamos recalcular predicciones rapidas para plotear\n",
    "X_train, X_test, y_train, y_test = train_classic_model(df, 'clean_text', 'category')[1]['X_test'], train_classic_model(df, 'clean_text', 'category')[1]['y_test'] # Solo para demo visual\n",
    "ConfusionMatrixDisplay.from_estimator(model_classic, X_test, y_test, ax=ax[1], cmap='Purples', colorbar=False)\n",
    "ax[1].set_title('Matriz de Confusión (Modelo Clásico)')\n",
    "ax[1].grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471bf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_product(text):\n",
    "    \"\"\"Función para probar el modelo manualmente\"\"\"\n",
    "    clean = normalize_text(text)\n",
    "    pred = model_classic.predict([clean])[0]\n",
    "    prob = model_classic.predict_proba([clean]).max()\n",
    "    return f\"Input: '{text}' -> Pred: {pred} ({prob:.1%})\"\n",
    "\n",
    "# Probemos con casos difíciles (Errores de dedo, abreviaturas)\n",
    "test_cases = [\n",
    "    \"LECH LALA ENTE 1L\",\n",
    "    \"JABN ZOT ROS 400G\",\n",
    "    \"COCA COL 600ML\",\n",
    "    \"SOPA NISI CAMRN\"\n",
    "]\n",
    "\n",
    "print(\"--- DEMOSTRACIÓN DE INFERENCIA ---\")\n",
    "for t in test_cases:\n",
    "    print(predict_product(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f4686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ed9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
